"""
Clear E-Commerce Cache and Re-Index

This script is part of the app.scripts module.

Run from backend directory:
    python -m app.scripts.clear_cache_and_reindex
"""

import asyncio

from app.services.scraping.scraping_cache_service import scraping_cache_service
from app.services.storage.pinecone_client import get_pinecone_index
from app.services.storage.supabase_client import get_supabase_client


async def clear_cache_and_reindex():
    """Clear cache and re-index ALL e-commerce URLs"""

    supabase = get_supabase_client()
    index = get_pinecone_index()

    print("=" * 80)
    print("ğŸ§¹ CLEAR CACHE + RE-INDEX ALL E-COMMERCE URLs")
    print("=" * 80)
    print()

    # Step 1: Find ALL e-commerce uploads
    print("Step 1: Finding ALL e-commerce uploads...")

    # Get ALL URL uploads
    result = supabase.table("uploads").select("*").eq("type", "url").execute()

    if not result.data:
        print("âŒ No URL uploads found in database")
        return

    # Filter for e-commerce URLs
    ecommerce_patterns = [
        "ambassadorscentworks.com",
        "shopify",
        "/products/",
        "/collections/",
        "/shop/",
        "/store/",
    ]

    uploads = []
    for upload in result.data:
        source = upload.get("source", "")
        if any(pattern in source.lower() for pattern in ecommerce_patterns):
            uploads.append(upload)

    if not uploads:
        print("âŒ No e-commerce uploads found")
        print("\nâ„¹ï¸  Upload an e-commerce URL through the dashboard first.")
        return

    print(f"âœ… Found {len(uploads)} e-commerce upload(s):")
    for i, upload in enumerate(uploads, 1):
        print(f"   {i}. {upload['source'][:80]}... (status: {upload['status']})")
    print()

    for upload in uploads:
        upload_id = upload["id"]
        org_id = upload["org_id"]
        namespace = upload["pinecone_namespace"]

        print(f"\n{'='*80}")
        print(f"Processing Upload: {upload_id}")
        print(f"Organization: {org_id}")
        print(f"Namespace: {namespace}")
        print(f"{'='*80}")

        # Step 2: CLEAR CACHE (THE KEY!)
        print("\nğŸ”¥ Step 2: Clearing cached scraped data...")
        source_url = upload["source"]
        try:
            deleted_count = await scraping_cache_service.invalidate_url(
                url=source_url, content_type="url", org_id=org_id
            )
            print(f"âœ… Cleared {deleted_count} cached entries")
            print(f"   Cache is now empty - next scrape will be FRESH!")
        except Exception as e:
            print(f"âš ï¸  Error clearing cache: {e}")
            print(f"   Continuing anyway...")

        # Step 3: Delete Pinecone vectors
        print("\nğŸ—‘ï¸  Step 3: Deleting old unstructured vectors from Pinecone...")
        try:
            index.delete(filter={"upload_id": upload_id}, namespace=namespace)
            print(f"âœ… Deleted old vectors for upload {upload_id}")
        except Exception as e:
            print(f"âš ï¸  Error deleting vectors: {e}")

        # Step 4: Mark for re-processing
        print("\nâ™»ï¸  Step 4: Marking upload for re-processing...")
        try:
            supabase.table("uploads").update(
                {"status": "pending", "error_message": None}
            ).eq("id", upload_id).execute()
            print(f"âœ… Upload {upload_id} marked as 'pending'")
        except Exception as e:
            print(f"âŒ Error updating upload: {e}")
            continue

        print(f"\nâœ… Upload {upload_id} ready for FRESH scrape with e-commerce scraper!")

    print("\n" + "=" * 80)
    print("âœ… CACHE CLEARED + RE-INDEX SETUP COMPLETE!")
    print("=" * 80)
    print(
        """
What Just Happened:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. âœ… CLEARED cached unstructured data (Redis/cache)
2. âœ… DELETED old vectors from Pinecone
3. âœ… MARKED uploads as 'pending' for re-processing

Next Steps:
â”â”â”â”â”â”â”â”â”â”
Backend worker will automatically pick up pending uploads.
Since cache is CLEARED, it will perform a FRESH scrape.
E-commerce scraper will detect the URL and extract structured data.

Monitor Progress:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    tail -f logs/app.log | grep "E-commerce"

Look for:
    Cache MISS - Performing fresh scrape  ğŸ‘ˆ KEY INDICATOR!
    [E-commerce] âœ… Structured scraping succeeded

Wait Time: 2-5 minutes

Then check Pinecone for structured data! ğŸ‰
    """
    )


if __name__ == "__main__":
    asyncio.run(clear_cache_and_reindex())
